---
---

@ARTICLE{93808,
  author={Freeman, W.T. and Adelson, E.H.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The design and use of steerable filters}, 
  year={1991},
  volume={13},
  number={9},
  pages={891-906},
  keywords={Adaptive filters;Nonlinear filters;Image edge detection;Image texture analysis;Image analysis;Adaptive control;Image sequence analysis;Image motion analysis;Shape;Image processing},
  doi={10.1109/34.93808}}

  @ARTICLE{1307008,
  author={Jacob, M. and Unser, M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Design of steerable filters for feature detection using canny-like criteria}, 
  year={2004},
  volume={26},
  number={8},
  pages={1007-1019},
  keywords={Computer vision;Detectors;Image edge detection;Nonlinear filters;Design methodology;Smoothing methods;Kernel;Jacobian matrices;Design optimization;Closed-form solution;Steerable;feature;edge;detection;ridge;contours;boundary;lines.},
  doi={10.1109/TPAMI.2004.44}}


@ARTICLE{Stirling2021-sg,
  title    = "{CellProfiler} 4: improvements in speed, utility and usability",
  author   = "Stirling, David R and Swain-Bowden, Madison J and Lucas, Alice M
              and Carpenter, Anne E and Cimini, Beth A and Goodman, Allen",
  abstract = "BACKGROUND: Imaging data contains a substantial amount of
              information which can be difficult to evaluate by eye. With the
              expansion of high throughput microscopy methodologies producing
              increasingly large datasets, automated and objective analysis of
              the resulting images is essential to effectively extract
              biological information from this data. CellProfiler is a free,
              open source image analysis program which enables researchers to
              generate modular pipelines with which to process microscopy
              images into interpretable measurements. RESULTS: Herein we
              describe CellProfiler 4, a new version of this software with
              expanded functionality. Based on user feedback, we have made
              several user interface refinements to improve the usability of
              the software. We introduced new modules to expand the
              capabilities of the software. We also evaluated performance and
              made targeted optimizations to reduce the time and cost
              associated with running common large-scale analysis pipelines.
              CONCLUSIONS: CellProfiler 4 provides significantly improved
              performance in complex workflows compared to previous versions.
              This release will ensure that researchers will have continued
              access to CellProfiler's powerful computational tools in the
              coming years.",
  journal  = "BMC Bioinformatics",
  volume   =  22,
  number   =  1,
  pages    = "433",
  month    =  sep,
  year     =  2021,
  keywords = "Bioimaging; Image analysis; Image quantitation; Image
              segmentation; Microscopy",
  language = "en"
}

@ARTICLE{Schindelin2012-kk,
  title    = "Fiji: an open-source platform for biological-image analysis",
  author   = "Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise,
              Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias
              and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan
              and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel
              James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak,
              Pavel and Cardona, Albert",
  abstract = "Fiji is a distribution of the popular open-source software ImageJ
              focused on biological-image analysis. Fiji uses modern software
              engineering practices to combine powerful software libraries with
              a broad range of scripting languages to enable rapid prototyping
              of image-processing algorithms. Fiji facilitates the
              transformation of new algorithms into ImageJ plugins that can be
              shared with end users through an integrated update system. We
              propose Fiji as a platform for productive collaboration between
              computer science and biology research communities.",
  journal  = "Nat. Methods",
  volume   =  9,
  number   =  7,
  pages    = "676--682",
  month    =  jun,
  year     =  2012,
  language = "en"
}

@ARTICLE{Tinevez2017-fb,
  title    = "{TrackMate}: An open and extensible platform for single-particle
              tracking",
  author   = "Tinevez, Jean-Yves and Perry, Nick and Schindelin, Johannes and
              Hoopes, Genevieve M and Reynolds, Gregory D and Laplantine,
              Emmanuel and Bednarek, Sebastian Y and Shorte, Spencer L and
              Eliceiri, Kevin W",
  abstract = "We present TrackMate, an open source Fiji plugin for the
              automated, semi-automated, and manual tracking of
              single-particles. It offers a versatile and modular solution that
              works out of the box for end users, through a simple and
              intuitive user interface. It is also easily scriptable and
              adaptable, operating equally well on 1D over time, 2D over time,
              3D over time, or other single and multi-channel image variants.
              TrackMate provides several visualization and analysis tools that
              aid in assessing the relevance of results. The utility of
              TrackMate is further enhanced through its ability to be readily
              customized to meet specific tracking problems. TrackMate is an
              extensible platform where developers can easily write their own
              detection, particle linking, visualization or analysis algorithms
              within the TrackMate environment. This evolving framework
              provides researchers with the opportunity to quickly develop and
              optimize new algorithms based on existing TrackMate modules
              without the need of having to write de novo user interfaces,
              including visualization, analysis and exporting tools. The
              current capabilities of TrackMate are presented in the context of
              three different biological problems. First, we perform
              Caenorhabditis-elegans lineage analysis to assess how
              light-induced damage during imaging impairs its early
              development. Our TrackMate-based lineage analysis indicates the
              lack of a cell-specific light-sensitive mechanism. Second, we
              investigate the recruitment of NEMO (NF-$\kappa$B essential
              modulator) clusters in fibroblasts after stimulation by the
              cytokine IL-1 and show that photodamage can generate artifacts in
              the shape of TrackMate characterized movements that confuse
              motility analysis. Finally, we validate the use of TrackMate for
              quantitative lifetime analysis of clathrin-mediated endocytosis
              in plant cells.",
  journal  = "Methods",
  volume   =  115,
  pages    = "80--90",
  month    =  feb,
  year     =  2017,
  keywords = "Clathin-mediated endocytosis; Image analysis; Microscopy;
              Open-source software; Phototoxicity; Single-particle tracking",
  language = "en"
}

@ARTICLE{Berg2019-no,
  title    = "ilastik: interactive machine learning for (bio)image analysis",
  author   = "Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and
              Straehle, Christoph N and Kausler, Bernhard X and Haubold,
              Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten
              and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I and Xu,
              Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong
              and Koethe, Ullrich and Hamprecht, Fred A and Kreshuk, Anna",
  abstract = "We present ilastik, an easy-to-use interactive tool that brings
              machine-learning-based (bio)image analysis to end users without
              substantial computational expertise. It contains pre-defined
              workflows for image segmentation, object classification, counting
              and tracking. Users adapt the workflows to the problem at hand by
              interactively providing sparse training annotations for a
              nonlinear classifier. ilastik can process data in up to five
              dimensions (3D, time and number of channels). Its computational
              back end runs operations on-demand wherever possible, allowing
              for interactive prediction on data larger than RAM. Once the
              classifiers are trained, ilastik workflows can be applied to new
              data from the command line without further user interaction. We
              describe all ilastik workflows in detail, including three case
              studies and a discussion on the expected performance.",
  journal  = "Nat. Methods",
  volume   =  16,
  number   =  12,
  pages    = "1226--1232",
  month    =  dec,
  year     =  2019,
  language = "en"
}


@INPROCEEDINGS{Krull2019-al,
  title     = "{Noise2Void} - Learning Denoising From Single Noisy Images",
  booktitle = "2019 {IEEE/CVF} Conference on Computer Vision and Pattern
               Recognition ({CVPR})",
  author    = "Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian",
  abstract  = "The field of image denoising is currently dominated by
               discriminative deep learning methods that are trained on pairs
               of noisy input and clean target images. Recently it has been
               shown that such methods can also be trained without clean
               targets. Instead, independent pairs of noisy images can be used,
               in an approach known as Noise2Noise (N2N). Here, we introduce
               Noise2Void (N2V), a training scheme that takes this idea one
               step further. It does not require noisy image pairs, nor clean
               target images. Consequently, N2V allows us to train directly on
               the body of data to be denoised and can therefore be applied
               when other methods cannot. Especially interesting is the
               application to biomedical image data, where the acquisition of
               training targets, clean or noisy, is frequently not possible. We
               compare the performance of N2V to approaches that have either
               clean target images and/or noisy image pairs available.
               Intuitively, N2V cannot be expected to outperform methods that
               have more information available during training. Still, we
               observe that the denoising performance of Noise2Void drops in
               moderation and compares favorably to training-free denoising
               methods.",
  publisher = "IEEE",
  pages     = "2124--2132",
  month     =  jun,
  year      =  2019
}

@ARTICLE{Von_Chamier2021-ux,
  title    = "Democratising deep learning for microscopy with {ZeroCostDL4Mic}",
  author   = "von Chamier, Lucas and Laine, Romain F and Jukkala, Johanna and
              Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and
              Lerche, Martina and Hern{\'a}ndez-P{\'e}rez, Sara and Mattila,
              Pieta K and Karinou, Eleni and Holden, S{\'e}amus and Solak,
              Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and
              Jones, Martin L and Royer, Lo{\"\i}c A and Leterrier, Christophe
              and Shechtman, Yoav and Jug, Florian and Heilemann, Mike and
              Jacquemet, Guillaume and Henriques, Ricardo",
  abstract = "Deep Learning (DL) methods are powerful analytical tools for
              microscopy and can outperform conventional image processing
              pipelines. Despite the enthusiasm and innovations fuelled by DL
              technology, the need to access powerful and compatible resources
              to train DL networks leads to an accessibility barrier that
              novice users often find difficult to overcome. Here, we present
              ZeroCostDL4Mic, an entry-level platform simplifying DL access by
              leveraging the free, cloud-based computational resources of
              Google Colab. ZeroCostDL4Mic allows researchers with no coding
              expertise to train and apply key DL networks to perform tasks
              including segmentation (using U-Net and StarDist), object
              detection (using YOLOv2), denoising (using CARE and Noise2Void),
              super-resolution microscopy (using Deep-STORM), and
              image-to-image translation (using Label-free prediction - fnet,
              pix2pix and CycleGAN). Importantly, we provide suitable
              quantitative tools for each network to evaluate model
              performance, allowing model optimisation. We demonstrate the
              application of the platform to study multiple biological
              processes.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "2276",
  month    =  apr,
  year     =  2021,
  language = "en"
}

@ARTICLE{Schmied2024-tl,
  title    = "Community-developed checklists for publishing images and image
              analyses",
  author   = "Schmied, Christopher and Nelson, Michael S and Avilov, Sergiy and
              Bakker, Gert-Jan and Bertocchi, Cristina and Bischof, Johanna and
              Boehm, Ulrike and Brocher, Jan and Carvalho, Mariana T and
              Chiritescu, Catalin and Christopher, Jana and Cimini, Beth A and
              Conde-Sousa, Eduardo and Ebner, Michael and Ecker, Rupert and
              Eliceiri, Kevin and Fernandez-Rodriguez, Julia and Gaudreault,
              Nathalie and Gelman, Laurent and Grunwald, David and Gu, Tingting
              and Halidi, Nadia and Hammer, Mathias and Hartley, Matthew and
              Held, Marie and Jug, Florian and Kapoor, Varun and Koksoy, Ayse
              Aslihan and Lacoste, Judith and Le D{\'e}v{\'e}dec, Sylvia and Le
              Guyader, Sylvie and Liu, Penghuan and Martins, Gabriel G and
              Mathur, Aastha and Miura, Kota and Montero Llopis, Paula and
              Nitschke, Roland and North, Alison and Parslow, Adam C and
              Payne-Dwyer, Alex and Plantard, Laure and Ali, Rizwan and
              Schroth-Diez, Britta and Sch{\"u}tz, Lucas and Scott, Ryan T and
              Seitz, Arne and Selchow, Olaf and Sharma, Ved P and Spitaler,
              Martin and Srinivasan, Sathya and Strambio-De-Castillia, Caterina
              and Taatjes, Douglas and Tischer, Christian and Jambor, Helena
              Klara",
  abstract = "Images document scientific discoveries and are prevalent in
              modern biomedical research. Microscopy imaging in particular is
              currently undergoing rapid technological advancements. However,
              for scientists wishing to publish obtained images and
              image-analysis results, there are currently no unified guidelines
              for best practices. Consequently, microscopy images and image
              data in publications may be unclear or difficult to interpret.
              Here, we present community-developed checklists for preparing
              light microscopy images and describing image analyses for
              publications. These checklists offer authors, readers and
              publishers key recommendations for image formatting and
              annotation, color selection, data availability and reporting
              image-analysis workflows. The goal of our guidelines is to
              increase the clarity and reproducibility of image figures and
              thereby to heighten the quality and explanatory power of
              microscopy data.",
  journal  = "Nat. Methods",
  volume   =  21,
  number   =  2,
  pages    = "170--181",
  month    =  feb,
  year     =  2024,
  language = "en"
}

@ARTICLE{Weigert2018-jw,
  title    = "Content-aware image restoration: pushing the limits of
              fluorescence microscopy",
  author   = "Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and
              M{\"u}ller, Andreas and Dibrov, Alexandr and Jain, Akanksha and
              Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and
              Culley, Si{\^a}n and Rocha-Martins, Mauricio and Segovia-Miranda,
              Fabi{\'a}n and Norden, Caren and Henriques, Ricardo and Zerial,
              Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel
              and Royer, Loic and Jug, Florian and Myers, Eugene W",
  abstract = "Fluorescence microscopy is a key driver of discoveries in the
              life sciences, with observable phenomena being limited by the
              optics of the microscope, the chemistry of the fluorophores, and
              the maximum photon exposure tolerated by the sample. These limits
              necessitate trade-offs between imaging speed, spatial resolution,
              light exposure, and imaging depth. In this work we show how
              content-aware image restoration based on deep learning extends
              the range of biological phenomena observable by microscopy. We
              demonstrate on eight concrete examples how microscopy images can
              be restored even if 60-fold fewer photons are used during
              acquisition, how near isotropic resolution can be achieved with
              up to tenfold under-sampling along the axial direction, and how
              tubular and granular structures smaller than the diffraction
              limit can be resolved at 20-times-higher frame rates compared to
              state-of-the-art methods. All developed image restoration methods
              are freely available as open source software in Python, FIJI, and
              KNIME.",
  journal  = "Nat. Methods",
  volume   =  15,
  number   =  12,
  pages    = "1090--1097",
  month    =  dec,
  year     =  2018,
  language = "en"
}

@ARTICLE{Senft2023-zy,
  title     = "A biologist's guide to planning and performing quantitative
               bioimaging experiments",
  author    = "Senft, Rebecca A and Diaz-Rohrer, Barbara and Colarusso, Pina
               and Swift, Lucy and Jamali, Nasim and Jambor, Helena and Pengo,
               Thomas and Brideau, Craig and Llopis, Paula Montero and Uhlmann,
               Virginie and Kirk, Jason and Gonzales, Kevin Andrew and
               Bankhead, Peter and Evans, III, Edward L and Eliceiri, Kevin W
               and Cimini, Beth A",
  abstract  = "Technological advancements in biology and microscopy have
               empowered a transition from bioimaging as an observational
               method to a quantitative one. However, as biologists are
               adopting quantitative bioimaging and these experiments become
               more complex, researchers need additional expertise to carry out
               this work in a rigorous and reproducible manner. This Essay
               provides a navigational guide for experimental biologists to aid
               understanding of quantitative bioimaging from sample preparation
               through to image acquisition, image analysis, and data
               interpretation. We discuss the interconnectedness of these
               steps, and for each, we provide general recommendations, key
               questions to consider, and links to high-quality open-access
               resources for further learning. This synthesis of information
               will empower biologists to plan and execute rigorous
               quantitative bioimaging experiments efficiently.",
  journal   = "PLoS Biol.",
  publisher = "Public Library of Science",
  volume    =  21,
  number    =  6,
  pages     = "e3002167",
  month     =  jun,
  year      =  2023
}

@ARTICLE{Rueden2019-qp,
  title     = "Scientific Community Image Forum: A discussion forum for
               scientific image software",
  author    = "Rueden, Curtis T and Ackerman, Jeanelle and Arena, Ellen T and
               Eglinger, Jan and Cimini, Beth A and Goodman, Allen and
               Carpenter, Anne E and Eliceiri, Kevin W",
  abstract  = "Forums and email lists play a major role in assisting scientists
               in using software. Previously, each open-source bioimaging
               software package had its own distinct forum or email list.
               Although each provided access to experts from various software
               teams, this fragmentation resulted in many scientists not
               knowing where to begin with their projects. Thus, the scientific
               imaging community lacked a central platform where solutions
               could be discussed in an open, software-independent manner. In
               response, we introduce the Scientific Community Image Forum,
               where users can pose software-related questions about digital
               image analysis, acquisition, and data management.",
  journal   = "PLoS Biol.",
  publisher = "Public Library of Science (PLoS)",
  volume    =  17,
  number    =  6,
  pages     = "e3000340",
  month     =  jun,
  year      =  2019,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Pietzsch2015-pf,
  title    = "{BigDataViewer}: visualization and processing for large image
              data sets",
  author   = "Pietzsch, Tobias and Saalfeld, Stephan and Preibisch, Stephan and
              Tomancak, Pavel",
  journal  = "Nat. Methods",
  volume   =  12,
  number   =  6,
  pages    = "481--483",
  month    =  jun,
  year     =  2015,
  language = "en"
}

@ARTICLE{Arzt2022-fs,
  title    = "{LABKIT}: Labeling and Segmentation Toolkit for Big Image Data",
  author   = "Arzt, Matthias and Deschamps, Joran and Schmied, Christopher and
              Pietzsch, Tobias and Schmidt, Deborah and Tomancak, Pavel and
              Haase, Robert and Jug, Florian",
  abstract = "We present LABKIT, a user-friendly Fiji plugin for the
              segmentation of microscopy image data. It offers easy to use
              manual and automated image segmentation routines that can be
              rapidly applied to single- and multi-channel images as well as to
              timelapse movies in 2D or 3D. LABKIT is specifically designed to
              work efficiently on big image data and enables users of consumer
              laptops to conveniently work with multiple-terabyte images. This
              efficiency is achieved by using ImgLib2 and BigDataViewer as well
              as a memory efficient and fast implementation of the random
              forest based pixel classification algorithm as the foundation of
              our software. Optionally we harness the power of graphics
              processing units (GPU) to gain additional runtime performance.
              LABKIT is easy to install on virtually all laptops and
              workstations. Additionally, LABKIT is compatible with high
              performance computing (HPC) clusters for distributed processing
              of big image data. The ability to use pixel classifiers trained
              in LABKIT via the ImageJ macro language enables our users to
              integrate this functionality as a processing step in automated
              image processing workflows. Finally, LABKIT comes with rich
              online resources such as tutorials and examples that will help
              users to familiarize themselves with available features and how
              to best use LABKIT in a number of practical real-world use-cases.",
  journal  = "Frontiers in Computer Science",
  volume   =  4,
  year     =  2022
}

@ARTICLE{Singh2014-yh,
  title    = "Pipeline for illumination correction of images for
              high-throughput microscopy",
  author   = "Singh, S and Bray, M-A and Jones, T R and Carpenter, A E",
  abstract = "The presence of systematic noise in images in high-throughput
              microscopy experiments can significantly impact the accuracy of
              downstream results. Among the most common sources of systematic
              noise is non-homogeneous illumination across the image field.
              This often adds an unacceptable level of noise, obscures true
              quantitative differences and precludes biological experiments
              that rely on accurate fluorescence intensity measurements. In
              this paper, we seek to quantify the improvement in the quality of
              high-content screen readouts due to software-based illumination
              correction. We present a straightforward illumination correction
              pipeline that has been used by our group across many experiments.
              We test the pipeline on real-world high-throughput image sets and
              evaluate the performance of the pipeline at two levels: (a)
              Z'-factor to evaluate the effect of the image correction on a
              univariate readout, representative of a typical high-content
              screen, and (b) classification accuracy on phenotypic signatures
              derived from the images, representative of an experiment
              involving more complex data mining. We find that applying the
              proposed post-hoc correction method improves performance in both
              experiments, even when illumination correction has already been
              applied using software associated with the instrument. To
              facilitate the ready application and future development of
              illumination correction methods, we have made our complete test
              data sets as well as open-source image analysis pipelines
              publicly available. This software-based solution has the
              potential to improve outcomes for a wide-variety of image-based
              HTS experiments.",
  journal  = "J. Microsc.",
  volume   =  256,
  number   =  3,
  pages    = "231--236",
  month    =  dec,
  year     =  2014,
  keywords = "Fluorescence microscopy; high-throughput microscopy; illumination
              correction; shading correction; vignetting",
  language = "en"
}


@ARTICLE{Gustafsdottir2013-ng,
  title    = "Multiplex cytological profiling assay to measure diverse cellular
              states",
  author   = "Gustafsdottir, Sigrun M and Ljosa, Vebjorn and Sokolnicki,
              Katherine L and Anthony Wilson, J and Walpita, Deepika and Kemp,
              Melissa M and Petri Seiler, Kathleen and Carrel, Hyman A and
              Golub, Todd R and Schreiber, Stuart L and Clemons, Paul A and
              Carpenter, Anne E and Shamji, Alykhan F",
  abstract = "Computational methods for image-based profiling are under active
              development, but their success hinges on assays that can capture
              a wide range of phenotypes. We have developed a multiplex
              cytological profiling assay that ``paints the cell'' with as many
              fluorescent markers as possible without compromising our ability
              to extract rich, quantitative profiles in high throughput. The
              assay detects seven major cellular components. In a pilot screen
              of bioactive compounds, the assay detected a range of cellular
              phenotypes and it clustered compounds with similar annotated
              protein targets or chemical structure based on cytological
              profiles. The results demonstrate that the assay captures subtle
              patterns in the combination of morphological labels, thereby
              detecting the effects of chemical compounds even though their
              targets are not stained directly. This image-based assay provides
              an unbiased approach to characterize compound- and
              disease-associated cell states to support future probe discovery.",
  journal  = "PLoS One",
  volume   =  8,
  number   =  12,
  pages    = "e80999",
  month    =  dec,
  year     =  2013,
  language = "en"
}


@ARTICLE{Ljosa2012-bf,
  title    = "Annotated high-throughput microscopy image sets for validation",
  author   = "Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E",
  journal  = "Nat. Methods",
  volume   =  9,
  number   =  7,
  pages    = "637",
  month    =  jun,
  year     =  2012,
  language = "en"
}


@ARTICLE{Reinke2024-aj,
  title    = "Understanding metric-related pitfalls in image analysis
              validation",
  author   = "Reinke, Annika and Tizabi, Minu D and Baumgartner, Michael and
              Eisenmann, Matthias and Heckmann-N{\"o}tzel, Doreen and Kavur, A
              Emre and R{\"a}dsch, Tim and Sudre, Carole H and Acion, Laura and
              Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis,
              Arriel and Buettner, Florian and Cardoso, M Jorge and Cheplygina,
              Veronika and Chen, Jianxu and Christodoulou, Evangelia and
              Cimini, Beth A and Farahani, Keyvan and Ferrer, Luciana and
              Galdran, Adrian and van Ginneken, Bram and Glocker, Ben and
              Godau, Patrick and Hashimoto, Daniel A and Hoffman, Michael M and
              Huisman, Merel and Isensee, Fabian and Jannin, Pierre and Kahn,
              Charles E and Kainmueller, Dagmar and Kainz, Bernhard and
              Karargyris, Alexandros and Kleesiek, Jens and Kofler, Florian and
              Kooi, Thijs and Kopp-Schneider, Annette and Kozubek, Michal and
              Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A and
              Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel,
              Anne L and Meijering, Erik and Menze, Bjoern and Moons, Karel G M
              and M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix
              and Petersen, Jens and Rafelski, Susanne M and Rajpoot, Nasir and
              Reyes, Mauricio and Riegler, Michael A and Rieke, Nicola and
              Saez-Rodriguez, Julio and S{\'a}nchez, Clara I and Shetty,
              Shravya and Summers, Ronald M and Taha, Abdel A and Tiulpin,
              Aleksei and Tsaftaris, Sotirios A and Van Calster, Ben and
              Varoquaux, Ga{\"e}l and Yaniv, Ziv R and J{\"a}ger, Paul F and
              Maier-Hein, Lena",
  abstract = "Validation metrics are key for tracking scientific progress and
              bridging the current chasm between artificial intelligence
              research and its translation into practice. However, increasing
              evidence shows that, particularly in image analysis, metrics are
              often chosen inadequately. Although taking into account the
              individual strengths, weaknesses and limitations of validation
              metrics is a critical prerequisite to making educated choices,
              the relevant knowledge is currently scattered and poorly
              accessible to individual researchers. Based on a multistage
              Delphi process conducted by a multidisciplinary expert consortium
              as well as extensive community feedback, the present work
              provides a reliable and comprehensive common point of access to
              information on pitfalls related to validation metrics in image
              analysis. Although focused on biomedical image analysis, the
              addressed pitfalls generalize across application domains and are
              categorized according to a newly created, domain-agnostic
              taxonomy. The work serves to enhance global comprehension of a
              key topic in image analysis validation.",
  journal  = "Nat. Methods",
  volume   =  21,
  number   =  2,
  pages    = "182--194",
  month    =  feb,
  year     =  2024,
  language = "en"
}

@ARTICLE{Maier-Hein2024-cp,
  title    = "Metrics reloaded: recommendations for image analysis validation",
  author   = "Maier-Hein, Lena and Reinke, Annika and Godau, Patrick and
              Tizabi, Minu D and Buettner, Florian and Christodoulou, Evangelia
              and Glocker, Ben and Isensee, Fabian and Kleesiek, Jens and
              Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A and
              Wiesenfarth, Manuel and Kavur, A Emre and Sudre, Carole H and
              Baumgartner, Michael and Eisenmann, Matthias and
              Heckmann-N{\"o}tzel, Doreen and R{\"a}dsch, Tim and Acion, Laura
              and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and
              Benis, Arriel and Blaschko, Matthew B and Cardoso, M Jorge and
              Cheplygina, Veronika and Cimini, Beth A and Collins, Gary S and
              Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and van
              Ginneken, Bram and Haase, Robert and Hashimoto, Daniel A and
              Hoffman, Michael M and Huisman, Merel and Jannin, Pierre and
              Kahn, Charles E and Kainmueller, Dagmar and Kainz, Bernhard and
              Karargyris, Alexandros and Karthikesalingam, Alan and Kofler,
              Florian and Kopp-Schneider, Annette and Kreshuk, Anna and Kurc,
              Tahsin and Landman, Bennett A and Litjens, Geert and Madani, Amin
              and Maier-Hein, Klaus and Martel, Anne L and Mattson, Peter and
              Meijering, Erik and Menze, Bjoern and Moons, Karel G M and
              M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix and
              Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and
              Saez-Rodriguez, Julio and S{\'a}nchez, Clara I and Shetty,
              Shravya and van Smeden, Maarten and Summers, Ronald M and Taha,
              Abdel A and Tiulpin, Aleksei and Tsaftaris, Sotirios A and Van
              Calster, Ben and Varoquaux, Ga{\"e}l and J{\"a}ger, Paul F",
  abstract = "Increasing evidence shows that flaws in machine learning (ML)
              algorithm validation are an underestimated global problem. In
              biomedical image analysis, chosen performance metrics often do
              not reflect the domain interest, and thus fail to adequately
              measure scientific progress and hinder translation of ML
              techniques into practice. To overcome this, we created Metrics
              Reloaded, a comprehensive framework guiding researchers in the
              problem-aware selection of metrics. Developed by a large
              international consortium in a multistage Delphi process, it is
              based on the novel concept of a problem fingerprint-a structured
              representation of the given problem that captures all aspects
              that are relevant for metric selection, from the domain interest
              to the properties of the target structure(s), dataset and
              algorithm output. On the basis of the problem fingerprint, users
              are guided through the process of choosing and applying
              appropriate validation metrics while being made aware of
              potential pitfalls. Metrics Reloaded targets image analysis
              problems that can be interpreted as classification tasks at
              image, object or pixel level, namely image-level classification,
              object detection, semantic segmentation and instance segmentation
              tasks. To improve the user experience, we implemented the
              framework in the Metrics Reloaded online tool. Following the
              convergence of ML methodology across application domains, Metrics
              Reloaded fosters the convergence of validation methodology. Its
              applicability is demonstrated for various biomedical use cases.",
  journal  = "Nat. Methods",
  volume   =  21,
  number   =  2,
  pages    = "195--212",
  month    =  feb,
  year     =  2024,
  language = "en"
}

@misc{reinke2023common,
      title={Common Limitations of Image Processing Metrics: A Picture Story}, 
      author={Annika Reinke and Minu D. Tizabi and Carole H. Sudre and Matthias Eisenmann and Tim Rädsch and Michael Baumgartner and Laura Acion and Michela Antonelli and Tal Arbel and Spyridon Bakas and Peter Bankhead and Arriel Benis and Matthew Blaschko and Florian Buettner and M. Jorge Cardoso and Jianxu Chen and Veronika Cheplygina and Evangelia Christodoulou and Beth Cimini and Gary S. Collins and Sandy Engelhardt and Keyvan Farahani and Luciana Ferrer and Adrian Galdran and Bram van Ginneken and Ben Glocker and Patrick Godau and Robert Haase and Fred Hamprecht and Daniel A. Hashimoto and Doreen Heckmann-Nötzel and Peter Hirsch and Michael M. Hoffman and Merel Huisman and Fabian Isensee and Pierre Jannin and Charles E. Kahn and Dagmar Kainmueller and Bernhard Kainz and Alexandros Karargyris and Alan Karthikesalingam and A. Emre Kavur and Hannes Kenngott and Jens Kleesiek and Andreas Kleppe and Sven Kohler and Florian Kofler and Annette Kopp-Schneider and Thijs Kooi and Michal Kozubek and Anna Kreshuk and Tahsin Kurc and Bennett A. Landman and Geert Litjens and Amin Madani and Klaus Maier-Hein and Anne L. Martel and Peter Mattson and Erik Meijering and Bjoern Menze and David Moher and Karel G. M. Moons and Henning Müller and Brennan Nichyporuk and Felix Nickel and M. Alican Noyan and Jens Petersen and Gorkem Polat and Susanne M. Rafelski and Nasir Rajpoot and Mauricio Reyes and Nicola Rieke and Michael Riegler and Hassan Rivaz and Julio Saez-Rodriguez and Clara I. Sánchez and Julien Schroeter and Anindo Saha and M. Alper Selver and Lalith Sharan and Shravya Shetty and Maarten van Smeden and Bram Stieltjes and Ronald M. Summers and Abdel A. Taha and Aleksei Tiulpin and Sotirios A. Tsaftaris and Ben Van Calster and Gaël Varoquaux and Manuel Wiesenfarth and Ziv R. Yaniv and Paul Jäger and Lena Maier-Hein},
      year={2023},
      eprint={2104.05642},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@ARTICLE{Lee2024-zu,
  title    = "Believing is seeing - the deceptive influence of bias in
              quantitative microscopy",
  author   = "Lee, Rachel M and Eisenman, Leanna R and Khuon, Satya and Aaron,
              Jesse S and Chew, Teng-Leong",
  abstract = "The visual allure of microscopy makes it an intuitively powerful
              research tool. Intuition, however, can easily obscure or distort
              the reality of the information contained in an image. Common
              cognitive biases, combined with institutional pressures that
              reward positive research results, can quickly skew a microscopy
              project towards upholding, rather than rigorously challenging, a
              hypothesis. The impact of these biases on a variety of research
              topics is well known. What might be less appreciated are the many
              forms in which bias can permeate a microscopy experiment. Even
              well-intentioned researchers are susceptible to bias, which must
              therefore be actively recognized to be mitigated. Importantly,
              although image quantification has increasingly become an
              expectation, ostensibly to confront subtle biases, it is not a
              guarantee against bias and cannot alone shield an experiment from
              cognitive distortions. Here, we provide illustrative examples of
              the insidiously pervasive nature of bias in microscopy
              experiments - from initial experimental design to image
              acquisition, analysis and data interpretation. We then provide
              suggestions that can serve as guard rails against bias.",
  journal  = "J. Cell Sci.",
  volume   =  137,
  number   =  1,
  month    =  jan,
  year     =  2024,
  keywords = "Bias; Bioimage analysis; Microscopy; Quantitative microscopy",
  language = "en"
}

@ARTICLE{Weisbart2023-kc,
  title    = "{CellProfiler} plugins - An easy image analysis platform
              integration for containers and Python tools",
  author   = "Weisbart, Erin and Tromans-Coia, Callum and Diaz-Rohrer, Barbara
              and Stirling, David R and Garcia-Fossa, Fernanda and Senft,
              Rebecca A and Hiner, Mark C and de Jesus, Marcelo B and Eliceiri,
              Kevin W and Cimini, Beth A",
  abstract = "CellProfiler is a widely used software for creating reproducible,
              reusable image analysis workflows without needing to code. In
              addition to the >90 modules that make up the main CellProfiler
              program, CellProfiler has a plugins system that allows for the
              creation of new modules which integrate with other Python tools
              or tools that are packaged in software containers. The
              CellProfiler-plugins repository contains a number of these
              CellProfiler modules, especially modules that are experimental
              and/or dependency-heavy. Here, we present an upgraded
              CellProfiler-plugins repository, an example of accessing
              containerised tools, improved documentation and added
              citation/reference tools to facilitate the use and contribution
              of the community.",
  journal  = "J. Microsc.",
  month    =  sep,
  year     =  2023,
  keywords = "CellProfiler; Python; image analysis; plugin; software; software
              container; workflow",
  language = "en"
}

@UNPUBLISHED{Stringer2024-gd,
  title    = "Cellpose3: one-click image restoration for improved cellular
              segmentation",
  author   = "Stringer, Carsen and Pachitariu, Marius",
  abstract = "Generalist methods for cellular segmentation have good
              out-of-the-box performance on a variety of image types. However,
              existing methods struggle for images that are degraded by noise,
              blurred or undersampled, all of which are common in microscopy.
              We focused the development of Cellpose3 on addressing these
              cases, and here we demonstrate substantial out-of-the-box gains
              in segmentation and image quality for noisy, blurry or
              undersampled images. Unlike previous approaches, which train
              models to restore pixel values, we trained Cellpose3 to output
              images that are well-segmented by a generalist segmentation
              model, while maintaining perceptual similarity to the target
              images. Furthermore, we trained the restoration models on a
              large, varied collection of datasets, thus ensuring good
              generalization to user images. We provide these tools as
              ``one-click'' buttons inside the graphical interface of Cellpose
              as well as in the Cellpose API. \#\#\# Competing Interest
              Statement The authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2024.02.10.579780",
  month    =  feb,
  year     =  2024,
  language = "en"
}

@UNPUBLISHED{Ouyang2022-fm,
  title    = "{BioImage} Model Zoo: A {Community-Driven} Resource for
              Accessible Deep Learning in {BioImage} Analysis",
  author   = "Ouyang, Wei and Beuttenmueller, Fynn and G{\'o}mez-de-Mariscal,
              Estibaliz and Pape, Constantin and Burke, Tom and
              Garcia-L{\'o}pez-de-Haro, Carlos and Russell, Craig and
              Moya-Sans, Luc{\'\i}a and de-la-Torre-Guti{\'e}rrez, Cristina and
              Schmidt, Deborah and Kutra, Dominik and Novikov, Maksim and
              Weigert, Martin and Schmidt, Uwe and Bankhead, Peter and
              Jacquemet, Guillaume and Sage, Daniel and Henriques, Ricardo and
              Mu{\~n}oz-Barrutia, Arrate and Lundberg, Emma and Jug, Florian
              and Kreshuk, Anna",
  abstract = "Deep learning-based approaches are revolutionizing imaging-driven
              scientific research. However, the accessibility and
              reproducibility of deep learning-based workflows for imaging
              scientists remain far from sufficient. Several tools have
              recently risen to the challenge of democratizing deep learning by
              providing user-friendly interfaces to analyze new data with
              pre-trained or fine-tuned models. Still, few of the existing
              pre-trained models are interoperable between these tools,
              critically restricting a model's overall utility and the
              possibility of validating and reproducing scientific analyses.
              Here, we present the BioImage Model Zoo (): a community-driven,
              fully open resource where standardized pre-trained models can be
              shared, explored, tested, and downloaded for further adaptation
              or direct deployment in multiple end user-facing tools (e.g.,
              ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic,
              CSBDeep). To enable everyone to contribute and consume the Zoo
              resources, we provide a model standard to enable
              cross-compatibility, a rich list of example models and practical
              use-cases, developer tools, documentation, and the accompanying
              infrastructure for model upload, download and testing. Our
              contribution aims to lay the groundwork to make deep learning
              methods for microscopy imaging findable, accessible,
              interoperable, and reusable (FAIR) across software tools and
              platforms. \#\#\# Competing Interest Statement The authors have
              declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2022.06.07.495102",
  month    =  jun,
  year     =  2022,
  language = "en"
}

@MISC{Goodman2021-xo,
  title  = "{Piximi - an images to discovery web platform}",
  author = "Goodman, Allen and Lucas, Alice and Gogoberidze, Nodar and Moser,
            Levin and Dao, David and Friedrich, Christoph and Paavolainen,
            Lassi and Papaleo, Andr{\'e}a and Stirling, David and Hubis,
            Frances and Molnar, Csaba and Ryder, Pearl and Wang, Rex and Hung,
            Jane and Horvath, Peter and Cimini, Beth and Carpenter, Anne",
  month  =  jun,
  year   =  2021
}

@ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}
